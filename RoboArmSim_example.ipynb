{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from torchrl.envs.utils import step_mdp\n",
    "from tqdm import tqdm\n",
    "from environments import make_env\n",
    "from src.agents import get_agent\n",
    "from src.utils import (\n",
    "    login,\n",
    "    logout,\n",
    "    prefill_buffer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameter\n",
    "Define training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_name: RoboArm-SAC-Example\n",
      "device: cuda\n",
      "episodes: 200\n",
      "agent:\n",
      "  name: sac\n",
      "  lr: 0.0003\n",
      "  batch_size: 256\n",
      "  num_updates: 1\n",
      "  prefill_episodes: 10\n",
      "  num_cells: 256\n",
      "  gamma: 0.99\n",
      "  soft_update_eps: 0.995\n",
      "  alpha_init: 1\n",
      "  fixed_alpha: false\n",
      "  loss_function: l2\n",
      "  normalization: None\n",
      "  dropout: 0.0\n",
      "  prb: 0\n",
      "  buffer_size: 1000000\n",
      "  reset_params: false\n",
      "env:\n",
      "  name: roboarm_sim-v0\n",
      "  max_episode_steps: 100\n",
      "  verbose: 0\n",
      "  frame_stack: 1\n",
      "  action_filter: 1\n",
      "  noise: 0.05\n",
      "  reward_signal: dense\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agent configuration\n",
    "agent_parameters = {\n",
    "    \"name\": \"sac\",\n",
    "    \"lr\": 3e-4,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_updates\": 1,\n",
    "    \"prefill_episodes\": 10,\n",
    "    \"num_cells\": 256,\n",
    "    \"gamma\": 0.99,\n",
    "    \"soft_update_eps\": 0.995,\n",
    "    \"alpha_init\": 1,\n",
    "    \"fixed_alpha\": False,\n",
    "    \"loss_function\": \"l2\",\n",
    "    \"normalization\": \"None\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"prb\": 0,\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"reset_params\": False,\n",
    "}\n",
    "\n",
    "# Environment configuration\n",
    "env_parameters = {\n",
    "    \"name\": \"roboarm_sim-v0\",\n",
    "    \"max_episode_steps\": 100,\n",
    "    \"verbose\": 0,\n",
    "    \"frame_stack\": 1,\n",
    "    \"action_filter\": 1,\n",
    "    \"noise\": 0.05,\n",
    "    \"reward_signal\": \"dense\",\n",
    "}\n",
    "conf = OmegaConf.create({\"run_name\": \"RoboArm-SAC-Example\",\n",
    "                         \"device\": \"cuda\",\n",
    "                         \"episodes\": 200,\n",
    "                         \"agent\": agent_parameters,\n",
    "                         \"env\": env_parameters,})\n",
    "print(OmegaConf.to_yaml(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent & Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Agent initialized ---\n",
      "Model not loaded!\n",
      "Buffer not loaded!\n"
     ]
    }
   ],
   "source": [
    "# create environment\n",
    "env, action_space, state_space = make_env(conf)\n",
    "\n",
    "# make agent\n",
    "agent, project_name = get_agent(action_space, state_space, conf)\n",
    "\n",
    "# loading agent weights or replay buffer\n",
    "login(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefill Replay Buffer\n",
    "Prefill the replay buffer with random action transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefill_buffer(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    num_episodes=conf.agent.prefill_episodes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = conf.agent.batch_size\n",
    "num_updates = conf.agent.num_updates\n",
    "env_name = conf.env.name\n",
    "train_episodes = conf.episodes\n",
    "max_episode_steps = conf.env.max_episode_steps\n",
    "\n",
    "# Lists for logging\n",
    "rewards = []\n",
    "final_errors = []\n",
    "steps = []\n",
    "\n",
    "for e in tqdm(range(train_episodes), desc=\"Training\"):\n",
    "    td = env.reset()\n",
    "    done = td.get(\"done\", False)\n",
    "    truncated = td.get(\"truncated\", False)\n",
    "    ep_return = 0\n",
    "    ep_steps = 0\n",
    "    total_step_times = []\n",
    "\n",
    "    while not done and not truncated:\n",
    "        ep_steps += 1\n",
    "        td = agent.get_action(td)\n",
    "        td = env.step(td)\n",
    "        agent.add_experience(td)\n",
    "        done = td.get((\"next\", \"done\"), False)\n",
    "        ep_return += td.get((\"next\", \"reward\"), 0)\n",
    "\n",
    "        td = step_mdp(td)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Train the agent\n",
    "    loss_info = agent.train(\n",
    "        batch_size=batch_size, num_updates=num_updates * ep_steps\n",
    "    )\n",
    "\n",
    "    # Metrics Logging\n",
    "    rewards.append(ep_return)\n",
    "    steps.append(ep_steps)\n",
    "    final_errors.append(td.get((\"error\"), 0).item())\n",
    "\n",
    "# Save agent weights or replay buffer\n",
    "logout(agent)\n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lego",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
